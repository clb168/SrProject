


 ###########  continuous-integration-polaris.md  ###########

Continuous Integration on PolarisContent is still being developed. Please check back.

 ###########  polaris-programming-models.md  ###########

Programming Models on PolarisThe software environment on Polaris supports several parallel programming models targeting the CPUs and GPUs.CPU Parallel Programming ModelsThe Cray compiler wrappers cc, CC, and ftn are recommended for MPI applications as they provide the needed include paths and libraries for each programming environment. A summary of available CPU parallel programming models and relevant compiler flags is shown below. Users are encouraged to review the corresponding man pages and documentation.|Programming Model| GNU | NVHPC | LLVM || --- | --- | --- | --- || OpenMP | -fopenmp | -mp | -fopenmp || OpenACC | -- | -acc=multicore | -- | GPU Programming ModelsA summary of available GPU programming models and relevant compiler flags is shown below for compilers that generate offloadable code. Users are encouraged to review the corresponding man pages and documentation.|Programming Model| GNU | NVHPC | LLVM | LLVM-SYCL || --- | --- | --- | --- | --- || CUDA | -- | -cuda [-gpu=cuda8.0,cc11.0] | -- | -- || HIP* | -- | -- | -- | -- || OpenACC | -- | -acc | -- | -- || OpenCL* | -- | -- | -- | -- || OpenMP | --| -mp=gpu | -fopenmp-targets=nvptx64 | -- || SYCL | -- | -- | -- | -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend '--cuda-gpu-arch=sm_80' |Note, the llvm and llvm-sycl modules are provided by ALCF to complement the compilers provided by the Cray PE on Polaris.OpenCL is supported, but does not require specific compiler flags per-se as the offloaded kernels are just-in-time compiled. Abstraction programming models, such as Kokkos, can be built on top of some of these programming models (see below). A HIP compiler supporting the A100 GPUs is still to be installed on Polaris.Mapping Programming Models to Polaris ModulesThe table below offers some suggestions for how to get started setting up your environment on Polaris depending on the programming language and model. Note, mixed C/C++ and Fortran applications should choose the programming environment for the Fortran compiler because of mpi.mod and similar incompatibilities between Fortran-generated files from different compilers. Several simple examples for testing the software environment on Polaris for different programming models are available in the ALCF GettingStart repo.Note, users are encouraged to use PrgEnv-nvhpc instead of PrgEnv-nvidia as the latter will soon be deprecated in Cray's PE. They are otherwise identical pointing to compilers from the same NVIDIA SDK version.|Programming Language| GPU Programming Model | Likely used Modules/Compilers | Notes || --- | --- | --- | --- || C/C++ | CUDA | PrgEnv-nvhpc, PrgEnv-gnu, llvm | NVIDIA (nvcc, nvc, nvc++) and clang compilers do GPU code generation || C/C++ | HIP | N/A | need to install with support for A100 || C/C++ | Kokkos | See CUDA | HIP, OpenMP, and SYCL/DPC++ also candidates || C/C++ | OpenACC | PrgEnv-nvhpc |  || C/C++ | OpenCL | PrgEnv-nvhpc, PrgEnv-gnu, llvm | JIT GPU code generation || C/C++ | OpenMP | PrgEnv-nvhpc, llvm |  || C/C++ | RAJA | See CUDA | HIP, OpenMP, and SYCL/DPC++ also candidates || C/C++ | SYCL/DPC++ | llvm-sycl |  || | | |  || Fortran | CUDA | PrgEnv-nvhpc | NVIDIA compiler (nvfortran) does GPU code generation; gfortran can be loaded via gcc-mixed || Fortran | HIP | N/A | need to install with support for A100 || Fortran | OpenACC | PrgEnv-nvhpc |  || Fortran | OpenCL | PrgEnv-nvhpc, PrgEnv-gnu | JIT GPU code generation || Fortran | OpenMP | PrgEnv-nvhpc |  |

 ###########  .DS_Store  ###########



 ###########  llvm-compilers-polaris.md  ###########

LLVM Compilers on PolarisThis page is not about LLVM-based Cray Compiling Environment (CCE) compilers from PrgEnv-cray but about open source LLVM compilers.If LLVM compilers are needed without MPI support, simply load the llvm or llvm-sycl module. Cray Programming Environment does not offer LLVM compiler support.Thus cc/CC/ftn compiler wrappers using LLVM compilers currently are not available.To use Clang with MPI, one can load the mpiwrappers/cray-mpich-llvm module which loads the following modules.
llvm, upstream llvm compilers

cray-mpich, MPI compiler wrappers mpicc/mpicxx/mpif90. mpif90 uses gfortran because flang is not ready for production use.

cray-pals, MPI launchers mpiexec/aprun/mpirun
Limitation There is no GPU-aware MPI library linking support by default. If needed, users should manually add the GTL (GPU Transport Layer) library to the application link line.OpenMP offloadWhen targeting the OpenMP or CUDA programming models for GPUs, the cudatoolkit-standalone module should also be loaded.SYCLFor users working with the SYCL programming model, a separate llvm module can be loaded in the environment with support for the A100 GPUs on Polaris.```module load llvm-sycl/2022-06```

 ###########  cce-compilers-polaris.md  ###########

CCE Compilers on PolarisThe Cray Compiling Environment (CCE) compilers are available on Polaris via the PrgEnv-cray module. The CCE compilers currently on Polaris only support AMD GPU targets for HIP and are thus not usable with the A100 GPUs. The nvhpc and llvm compilers can be used for compiling GPU-enabled applications.

 ###########  nvidia-compiler-polaris.md  ###########

NVIDIA Compilers on PolarisThe NVIDIA compilers (nvc, nvc++, nvcc, and nvfortran) are available on Polaris via the PrgEnv-nvhpc and nvhpc modules. There is currently a PrgEnv-nvidia module available, but that will soon be deprecated in Cray's PE, thus it is not recommend for use.The Cray compiler wrappers map to NVIDIA compilers as follows.```cc -> nvcCC -> nvc++ftn -> nvfortran```Users are encouraged to look through (NVIDIA's documentation)[https://developer.nvidia.com/hpc-sdk] for the NVHPC SDK and specific information on the compilers, tools, and libraries.Notes on NVIDIA CompilersPGI compilersThe NVIDIA programming environments makes available compilers from the NVIDIA HPC SDK. While the PGI compilers are available in this programming environment, it should be noted they are actually symlinks to the corresponding NVIDIA compilers.```pgcc -> nvcpgc++ -> nvc++pgf90 -> nvfortranpgfortran -> nvfortran```While nvcc is the traditional CUDA C and CUDA C++ compiler for NVIDIA GPUs, the nvc, nvc++, and nvfortran compilers additionally target CPUs.NVHPC SDK Directory StructureUsers migrating from CUDA toolkits to the NVHPC SDK may find it beneficial to review the directory structure of the hpc-sdk directory to find the location of commonly used libraries (including math libraries for the CPU). With the PrgEnv-nvhpc module loaded, the NVIDIA_PATH environment variable can be used to locate the path to various NVIDIA tools, libraries, and examples.
compiler/bin - cuda-gdb, ncu, nsys, ...

examples - CUDA-Fortran, OpenMP, ...

comm_libs - nccl, nvshmem, ...

compiler/libs - blas, lapack, ...

cuda/lib64 - cudart, OpenCL, ...

math_libs/lib64 - cublas, cufft, ...
Differences between nvcc and nvc/nvc++For users that want to continue using nvcc it is important to be mindful of differences with the newer nvc and nvc++ compilers. For example, the -cuda flag instructs nvcc to compile .cu input files to .cu.cpp.ii output files which are to be separately compiled, whereas the same -cuda flag instructs nvc, nvc++, and nvfortran to enable CUDA C/C++ or CUDA Fortran code generation. The resulting output file in each case is different (text vs. object) and one may see unrecognized format error when -cuda is incorrectly passed to nvcc.

 ###########  files  ###########



 ###########  gnu-compilers-polaris.md  ###########

GNU Compilers on PolarisThe GNU compilers are available on Polaris via the PrgEnv-gnu and gcc-mixed modules. The gcc-mixed module can be useful when, for example, the PrgEnv-nvhpc compilers are used to compile C/C++ MPI-enabled code and gfortran is needed.The GNU compilers currently on Polaris do not support GPU code generation and thus can only be used for compiling CPU codes.The nvhpc and llvm compilers can be used for compiling GPU-enabled applications.

 ###########  polaris-example-program-makefile.md  ###########

Example Programs and Makefiles for PolarisSeveral simple examples of building CPU and GPU-enabled codes on Polaris are available in the ALCF GettingStart repo for several programming models. If build your application is problematic for some reason (e.g. absence of a GPU), then users are encouraged to build and test applications directly on one of the Polaris compute nodes via an interactive job. The discussion below makes use of the NVHPC compilers in the default environment as illustrative examples. Similar examples for other compilers on Polaris are available in the ALCF GettingStarted repo.CPU MPI+OpenMP ExampleOne of the first useful tasks with any new machine, scheduler, and job launcher is to ensure one is binding MPI ranks and OpenMP threads to the host cpu as intended. A simple HelloWorld MPI+OpenMP example is available here to get started with.The application can be straightforwardly compiled using the Cray compiler wrappers.```CC -fopenmp main.cpp -o hello_affinity```The executable hello_affinity can then be launched in a job script (or directly in shell of interactive job) using mpiexec as discussed here.```!/bin/shPBS -l select=1:system=polarisPBS -l place=scatterPBS -l walltime=0:30:00MPI example w/ 16 MPI ranks per node spread evenly across coresNNODES=wc -l < $PBS_NODEFILENRANKS_PER_NODE=16NDEPTH=4NTHREADS=1NTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))echo "NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}"mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity```CUDASeveral variants of C/C++ and Fortran CUDA examples are available here that include MPI and multi-gpu examples.One can use the Cray compiler wrappers to compile GPU-enabled applications as well. This example of simple vector addition uses the NVIDIA compilers.```CC -g -O3 -std=c++0x -cuda main.cpp -o vecadd```The craype-accel-nvidia80 module in the default environment will add the -gpu compiler flag for nvhpc compilers along with appropriate include directories and libraries. It is left to the user to provide an additional flag to the nvhpc compilers to select the target GPU programming model. In this case, -cuda is used to indicate compilation of CUDA code. The application can then be launched within a batch job submission script or as follows on one of the compute nodes.```$ ./vecadd of devices= 4[0] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ][1] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ][2] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ][3] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]Running on GPU 0!Using single-precisionName= NVIDIA A100-SXM4-40GBLocally unique identifier= Clock Frequency(KHz)= 1410000Compute Mode= 0Major compute capability= 8Minor compute capability= 0Number of multiprocessors on device= 108Warp size in threads= 32Single precision performance ratio= 2Result is CORRECT!! :)```GPU OpenACCA simple MPI-parallel OpenACC example is available here. Compilation proceeds similar to the above CUDA example except for the use of the -acc=gpu compiler flag to indicate compilation of OpenACC code for GPUs.```CC -g -O3 -std=c++0x -acc=gpu -gpu=cc80,cuda11.0 main.cpp -o vecadd```In this example, each MPI rank sees all four GPUs on a Polaris node and GPUs are bound to MPI ranks round-robin within the application.```$ mpiexec -n 4 ./vecaddof devices= 4Using single-precisionRank 0 running on GPU 0!Rank 1 running on GPU 1!Rank 2 running on GPU 2!Rank 3 running on GPU 3!Result is CORRECT!! :)```If the application instead relies on the job launcher to bind MPI ranks to available GPUs, then a small helper script can be used to explicitly set CUDA_VISIBLE_DEVICES appropriately for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment. The binding of MPI ranks to GPUs is discussed in more detail here.GPU OpenCLA simple OpenCL example is available here. The OpenCL headers and library are available in the NVHPC SDK and cuda toolkits. The environment variable NVIDIA_PATH is defined for the PrgEnv-nvhpc programming environment. ```CC -o vecadd -g -O3 -std=c++0x  -I${NVIDIA_PATH}/cuda/include main.o -L${NVIDIA_PATH}/cuda/lib64 -lOpenCL```This simple example can be run on a Polaris compute node as follows.```$ ./vecaddRunning on GPU!Using single-precisionCL_DEVICE_NAME: NVIDIA A100-SXM4-40GB
CL_DEVICE_VERSION: OpenCL 3.0 CUDA
CL_DEVICE_OPENCL_C_VERSION: OpenCL C 1.2
CL_DEVICE_MAX_COMPUTE_UNITS: 108
CL_DEVICE_MAX_CLOCK_FREQUENCY: 1410
CL_DEVICE_MAX_WORK_GROUP_SIZE: 1024
Result is CORRECT!! :)```GPU OpenMPA simple MPI-parallel OpenMP example is available here. Compilation proceeds similar to the above examples except for use of the -mp=gpu compiler flag to indicated compilation of OpenMP code for GPUs.```CC -g -O3 -std=c++0x -mp=gpu -gpu=cc80,cuda11.0 -c main.cpp -o vecadd```Similar to the OpenACC example above, this code binds MPI ranks to GPUs in a round-robin fashion. ```$ mpiexec -n 4 ./vecaddof devices= 4Rank 0 running on GPU 0!Rank 1 running on GPU 1!Rank 2 running on GPU 2!Rank 3 running on GPU 3!Result is CORRECT!! :)```

 ###########  compiling-and-linking-overview.md  ###########

Compiling and Linking Overview on PolarisPolaris NodesLogin NodesThe login nodes do not currently have GPUs installed. It is still possible to compile GPU-enabled applications on the login nodes depending on the requirements of your applications build system. If a GPU is required for compilation, then users are encouraged for the time being to build their applications on a Polaris compute node. This can be readily accomplished by submitting an interactive single-node job. Compilation of non-GPU codes is expected to work well on the current Polaris login nodes.Home File SystemIs it helpful to realize that there is a single HOME filesystem for users that can be accessed from the login and computes of each production resource at ALCF. Thus, users should be mindful of modifications to their environments (e.g. .bashrc) that may cause issues to arise due to differences between the systems. An example is creating an alias for the qstat command to, for example, change the order of columns printed to screen. Users with such an alias that works well on Theta may run into issues using qstat on Polaris as the two system use different schedulers: Cobalt (Theta) and PBS (Polaris). Users with such modifications to their environments are encouraged to modify their scripts appropriately depending on $hostname.Interactive Jobs on Compute NodesSubmitting a single-node interactive job to, for example, build and test applications on a Polaris compute node can be accomplished using the qsub command.```qsub -I -l select=1 -l walltime=1:00:00```This command requests 1 node for a period of 1 hour. After waiting in the queue for a node to become available, a shell prompt on a compute node will become available. Users can then proceed to start building applications and testing job submission scripts.Cray Programming EnvironmentThe Cray Programming Environment (PE) uses three compiler wrappers for building software. These compiler wrappers should be used when building MPI-enabled applications.
cc - C compiler

CC - C++ compiler

ftn - Fortran compiler
Each of these wrappers can select a specific vendor compiler based on the PrgEnv module loaded in the environment. The following are some helpful options to understand what the compiler wrapper is invoking.
--craype-verbose : Print the command which is forwarded to the compiler invocation

--cray-print-opts=libs : Print library information

--cray-print-opts=cflags : Print include information
The output from these commands may be useful in build scripts where a compiler other than that invoked by a compiler wrapper is desired. Defining some variables as such may prove useful in those situations.```CRAY_CFLAGS=$(cc --cray-print-opts=cflags)CRAY_LIB=$(cc --cray-print-opts=libs)```Further documentation and options are available via man cc and similar. Compilers provided by Cray Programming EnvironmentsThe default programming environment on Polaris is currently NVHPC. The GNU compilers are available via another programming environment. The following sequence of module commands can be used to switch to the GNU programming environment (gcc, g++, gfortran) and also have NVIDIA compilers available in your path.```module swap PrgEnv-nvhpc PrgEnv-gnumodule load nvhpc-mixed```The compilers invoked by the Cray MPI wrappers are listed for each programming environment in the following table.|module| C | C++ | Fortran || --- | --- | --- | --- || MPI Compiler Wrapper | cc | CC | ftn || PrgEnv-nvhpc | nvc | nvc++ | nvfortran || PrgEnv-gnu | gcc | g++ | gfortran |Note, while gcc and g++ may be available in the default environment, the PrgEnv-gnu module is needed to provide gfortran.Additional Compilers Provided by ALCFThe ALCF additionally provides compilers to enable the OpenMP and SYCL programming models for GPUs viaLLVM as documented hereAdditional documentation for using compilers is available on the respective programming model pages: OpenMP and SYCL.LinkingDynamic linking of libraries is currently the default on Polaris. The Cray MPI wrappers will handle this automatically.Notes on Default Modules
craype-x86-rome: While the Polaris compute nodes currently have Milan CPUs, this module is loaded by default to avoid the craype-x86-milan module from adding a zen3 target not supported in the default nvhpc/21.9 compilers. The craype-x86-milan module is expected to be made default once a newer nvhpc version (e.g. 22.5) is made the default.

craype-accel-nvidia80: This module adds compiler flags to enable GPU acceleration for NVHPC compilers along with gpu-enabled MPI libraries as it is assumed that the majority of applications to be compiled on Polaris will target the GPUs for acceleration. Users building cpu-only applications may find it useful to unload this module to silence "gpu code generation" warnings.
Mixed C/C++ & Fortran ApplicationsFor applications consisting of a mix of C/C++ and Fortran that also uses MPI, it is suggested that the programming environment chosen for Fortran be used to build the full application because of mpi.mod (and similar) incompatibilities. Compiling for GPUsIt is assumed the majority of applications to be built on Polaris will make use of the GPUs. As such, the craype-accel-nvidia80 module is in the default environment. This has the effect of the Cray compiler wrappers adding -gpu to the compiler invocation along with additional include paths and libraries. Additional compilers flags may be needed depending on the compiler and GPU programming model used (e.g. -cuda, -acc, or -mp=gpu).This module also adds GPU Transport Layer (GTL) libraries to the link-line to support GPU-aware MPI applications. Note, there is currently an issue in the early Polaris software environment that may prevent applications from using GPU-enabled MPI.Man PagesFor additional information on the Cray wrappers, please refer to the man pages.```man ccman CCman ftn```